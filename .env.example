# ========================================
# Kubernetes AIOps Evidence Graph Platform
# Environment Configuration
# ========================================

# Application
APP_NAME=kubernetes-aiops-evidence-graph
APP_ENV=development
DEBUG=true
LOG_LEVEL=INFO

# API Server
API_HOST=0.0.0.0
API_PORT=8000
API_WORKERS=4

# ========================================
# Database Configuration
# ========================================

# PostgreSQL
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=aiops
POSTGRES_USER=aiops
POSTGRES_PASSWORD=aiops_secure_password_change_me
DATABASE_URL=postgresql+asyncpg://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB}

# Neo4j Graph Database
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=neo4j_secure_password_change_me

# Redis
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_PASSWORD=
REDIS_URL=redis://${REDIS_HOST}:${REDIS_PORT}/0

# ========================================
# Temporal Workflow Engine
# ========================================
TEMPORAL_HOST=localhost
TEMPORAL_PORT=7233
TEMPORAL_NAMESPACE=aiops
TEMPORAL_TASK_QUEUE=incident-workflow

# ========================================
# Kubernetes Configuration
# ========================================
# Leave empty to use in-cluster config or ~/.kube/config
KUBECONFIG=
KUBERNETES_DEFAULT_NAMESPACE=default

# ========================================
# Observability Stack
# ========================================

# Prometheus
PROMETHEUS_URL=http://localhost:9090

# Loki (Log Aggregation)
LOKI_URL=http://localhost:3100

# Grafana
GRAFANA_URL=http://localhost:3000
GRAFANA_API_KEY=

# OpenTelemetry
OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317
OTEL_SERVICE_NAME=aiops-platform
OTEL_TRACES_EXPORTER=otlp
OTEL_METRICS_EXPORTER=prometheus

# ========================================
# LLM Configuration
# ========================================
# Choose: openai, gemini, ollama
LLM_PROVIDER=gemini

# OpenAI
OPENAI_API_KEY=
OPENAI_MODEL=gpt-4o-mini

# Google Gemini
GOOGLE_API_KEY=
GEMINI_MODEL=gemini-3-pro

# Ollama (Local)
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=llama2

# ========================================
# Policy Engine (OPA)
# ========================================
OPA_URL=http://localhost:8181
OPA_POLICY_PATH=/v1/data/remediation

# ========================================
# Integrations
# ========================================

# Slack (ChatOps)
SLACK_BOT_TOKEN=
SLACK_SIGNING_SECRET=
SLACK_APPROVAL_CHANNEL=

# Jira
JIRA_URL=
JIRA_USER=
JIRA_API_TOKEN=
JIRA_PROJECT_KEY=

# ========================================
# Security & Rate Limiting
# ========================================
API_KEY_HEADER=X-API-Key
RATE_LIMIT_PER_MINUTE=100
CORS_ORIGINS=["http://localhost:3000","http://localhost:8080"]

# ========================================
# Evidence Collection
# ========================================
EVIDENCE_TIME_WINDOW_MINUTES=15
MAX_LOG_LINES=1000
MAX_METRIC_POINTS=500

# ========================================
# Remediation Settings
# ========================================
REMEDIATION_AUTO_APPROVE_DEV=true
REMEDIATION_AUTO_APPROVE_STAGING=false
REMEDIATION_AUTO_APPROVE_PROD=false
REMEDIATION_MAX_BLAST_RADIUS=50
REMEDIATION_VERIFICATION_WAIT_SECONDS=120
