groups:
  - name: kubernetes-pod-alerts
    interval: 30s
    rules:
      # CrashLoopBackOff detection
      - alert: PodCrashLooping
        expr: increase(kube_pod_container_status_restarts_total[15m]) > 3
        for: 5m
        labels:
          severity: critical
          category: crashloop
        annotations:
          summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is crash looping"
          description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} has restarted {{ $value }} times in the last 15 minutes"
          runbook_url: "https://runbooks.example.com/crashloop"

      # OOMKilled detection
      - alert: ContainerOOMKilled
        expr: kube_pod_container_status_last_terminated_reason{reason="OOMKilled"} == 1
        for: 0m
        labels:
          severity: critical
          category: oom
        annotations:
          summary: "Container {{ $labels.container }} in pod {{ $labels.pod }} was OOMKilled"
          description: "Container {{ $labels.container }} in pod {{ $labels.namespace }}/{{ $labels.pod }} was killed due to OOM"
          runbook_url: "https://runbooks.example.com/oomkilled"

      # ImagePullBackOff detection
      - alert: PodImagePullBackOff
        expr: kube_pod_container_status_waiting_reason{reason="ImagePullBackOff"} == 1
        for: 5m
        labels:
          severity: warning
          category: imagepull
        annotations:
          summary: "Pod {{ $labels.pod }} cannot pull image"
          description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is in ImagePullBackOff state"
          runbook_url: "https://runbooks.example.com/imagepull"

      # Pod not ready
      - alert: PodNotReady
        expr: kube_pod_status_ready{condition="false"} == 1
        for: 10m
        labels:
          severity: warning
          category: readiness
        annotations:
          summary: "Pod {{ $labels.pod }} is not ready"
          description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in a non-ready state for more than 10 minutes"

  - name: kubernetes-resource-alerts
    interval: 30s
    rules:
      # High memory usage
      - alert: ContainerHighMemoryUsage
        expr: (container_memory_usage_bytes / container_spec_memory_limit_bytes) > 0.9
        for: 5m
        labels:
          severity: warning
          category: resource
        annotations:
          summary: "Container {{ $labels.container }} memory usage is above 90%"
          description: "Container {{ $labels.container }} in {{ $labels.namespace }}/{{ $labels.pod }} is using {{ $value | humanizePercentage }} of its memory limit"

      # High CPU throttling
      - alert: ContainerCPUThrottling
        expr: rate(container_cpu_cfs_throttled_seconds_total[5m]) > 0.25
        for: 5m
        labels:
          severity: warning
          category: resource
        annotations:
          summary: "Container {{ $labels.container }} is being CPU throttled"
          description: "Container {{ $labels.container }} in {{ $labels.namespace }}/{{ $labels.pod }} is experiencing significant CPU throttling"

      # HPA at max capacity
      - alert: HPAAtMaxCapacity
        expr: kube_horizontalpodautoscaler_status_current_replicas == kube_horizontalpodautoscaler_spec_max_replicas
        for: 15m
        labels:
          severity: warning
          category: scaling
        annotations:
          summary: "HPA {{ $labels.horizontalpodautoscaler }} is at max capacity"
          description: "HPA {{ $labels.horizontalpodautoscaler }} in namespace {{ $labels.namespace }} has been at max replicas for 15 minutes"

  - name: kubernetes-node-alerts
    interval: 30s
    rules:
      # Node not ready
      - alert: NodeNotReady
        expr: kube_node_status_condition{condition="Ready", status="false"} == 1
        for: 5m
        labels:
          severity: critical
          category: node
        annotations:
          summary: "Node {{ $labels.node }} is not ready"
          description: "Node {{ $labels.node }} has been in a not ready state for more than 5 minutes"

      # Node disk pressure
      - alert: NodeDiskPressure
        expr: kube_node_status_condition{condition="DiskPressure", status="true"} == 1
        for: 5m
        labels:
          severity: warning
          category: node
        annotations:
          summary: "Node {{ $labels.node }} has disk pressure"
          description: "Node {{ $labels.node }} is experiencing disk pressure"

      # Node memory pressure
      - alert: NodeMemoryPressure
        expr: kube_node_status_condition{condition="MemoryPressure", status="true"} == 1
        for: 5m
        labels:
          severity: warning
          category: node
        annotations:
          summary: "Node {{ $labels.node }} has memory pressure"
          description: "Node {{ $labels.node }} is experiencing memory pressure"

  - name: application-alerts
    interval: 30s
    rules:
      # High error rate
      - alert: HighErrorRate
        expr: sum(rate(http_requests_total{status=~"5.."}[5m])) by (namespace, service) / sum(rate(http_requests_total[5m])) by (namespace, service) > 0.05
        for: 5m
        labels:
          severity: critical
          category: error_rate
        annotations:
          summary: "High error rate for {{ $labels.service }}"
          description: "Service {{ $labels.namespace }}/{{ $labels.service }} has an error rate of {{ $value | humanizePercentage }}"

      # High latency
      - alert: HighLatency
        expr: histogram_quantile(0.99, sum(rate(http_request_duration_seconds_bucket[5m])) by (le, namespace, service)) > 1
        for: 5m
        labels:
          severity: warning
          category: latency
        annotations:
          summary: "High latency for {{ $labels.service }}"
          description: "Service {{ $labels.namespace }}/{{ $labels.service }} has p99 latency of {{ $value }}s"

  - name: aiops-platform-alerts
    interval: 30s
    rules:
      # Workflow failures
      - alert: TemporalWorkflowFailures
        expr: increase(temporal_workflow_failed_total[15m]) > 5
        for: 5m
        labels:
          severity: warning
          category: platform
        annotations:
          summary: "High rate of Temporal workflow failures"
          description: "{{ $value }} workflow failures in the last 15 minutes"

      # Evidence collection timeout
      - alert: EvidenceCollectionSlow
        expr: histogram_quantile(0.95, rate(aiops_evidence_collection_duration_seconds_bucket[5m])) > 60
        for: 5m
        labels:
          severity: warning
          category: platform
        annotations:
          summary: "Evidence collection is slow"
          description: "P95 evidence collection time is {{ $value }}s"
